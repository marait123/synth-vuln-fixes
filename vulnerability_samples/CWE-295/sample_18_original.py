import aiohttp
import ssl
import asyncio
import logging
from dataclasses import dataclass
from typing import List, Dict
import argparse
import json
import csv
from datetime import datetime

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class SiteConfig:
    url: str
    keywords: List[str]

class DataProcessor:
    @staticmethod
    def process_html(html: str, keywords: List[str]) -> Dict[str, int]:
        return {keyword: html.lower().count(keyword.lower()) for keyword in keywords}

    @staticmethod
    def sanitize_input(text: str) -> str:
        return ''.join(char for char in text if char.isalnum() or char.isspace())

class Reporter:
    @staticmethod
    def generate_report(site: str, keyword_counts: Dict[str, int]) -> str:
        report = f"Report for {site}:\n"
        for keyword, count in keyword_counts.items():
            report += f"- '{keyword}' found {count} times\n"
        return report

    @staticmethod
    def save_report_to_csv(filename: str, data: List[Dict[str, any]]):
        with open(filename, 'w', newline='') as csvfile:
            fieldnames = ['url', 'keyword', 'count', 'timestamp']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            for item in data:
                for keyword, count in item['keyword_counts'].items():
                    writer.writerow({
                        'url': item['url'],
                        'keyword': keyword,
                        'count': count,
                        'timestamp': item['timestamp']
                    })

class SiteManager:
    def __init__(self, config_file: str):
        self.config_file = config_file
        self.sites = self.load_config()

    def load_config(self) -> List[SiteConfig]:
        try:
            with open(self.config_file, 'r') as f:
                config = json.load(f)
            return [SiteConfig(**site) for site in config['sites']]
        except (json.JSONDecodeError, FileNotFoundError) as e:
            logger.error(f"Error loading config file: {e}")
            return []

    def add_site(self, url: str, keywords: List[str]):
        self.sites.append(SiteConfig(url, keywords))
        self.save_config()

    def remove_site(self, url: str):
        self.sites = [site for site in self.sites if site.url != url]
        self.save_config()

    def save_config(self):
        config = {'sites': [{'url': site.url, 'keywords': site.keywords} for site in self.sites]}
        with open(self.config_file, 'w') as f:
            json.dump(config, f, indent=2)

async def fetch(session, url):
    ssl_context = ssl._create_unverified_context()
    try:
        async with session.get(url, ssl=ssl_context) as response:
            return await response.text()
    except aiohttp.ClientError as e:
        logger.error(f"Error fetching {url}: {e}")
        return ""

async def process_site(session, config: SiteConfig):
    html = await fetch(session, config.url)
    if html:
        keyword_counts = DataProcessor.process_html(html, config.keywords)
        report = Reporter.generate_report(config.url, keyword_counts)
        logger.info(report)
        return {
            'url': config.url,
            'keyword_counts': keyword_counts,
            'timestamp': datetime.now().isoformat()
        }
    else:
        logger.warning(f"No data fetched for {config.url}")
        return None

async def main(config_file: str, output_file: str):
    site_manager = SiteManager(config_file)
    
    async with aiohttp.ClientSession() as session:
        tasks = [process_site(session, site) for site in site_manager.sites]
        results = await asyncio.gather(*tasks)
        
    results = [r for r in results if r is not None]
    Reporter.save_report_to_csv(output_file, results)
    logger.info(f"Report saved to {output_file}")

def cli():
    parser = argparse.ArgumentParser(description="Web Scraper and Keyword Counter")
    parser.add_argument("--config", default="config.json", help="Path to the configuration file")
    parser.add_argument("--output", default="report.csv", help="Path to the output CSV file")
    parser.add_argument("--add-site", nargs=2, metavar=("URL", "KEYWORDS"), help="Add a new site to the configuration")
    parser.add_argument("--remove-site", metavar="URL", help="Remove a site from the configuration")
    
    args = parser.parse_args()
    
    site_manager = SiteManager(args.config)
    
    if args.add_site:
        url, keywords = args.add_site
        keywords = DataProcessor.sanitize_input(keywords).split()
        site_manager.add_site(url, keywords)
        logger.info(f"Added site: {url} with keywords: {keywords}")
    elif args.remove_site:
        site_manager.remove_site(args.remove_site)
        logger.info(f"Removed site: {args.remove_site}")
    else:
        asyncio.run(main(args.config, args.output))

if __name__ == "__main__":
    cli()